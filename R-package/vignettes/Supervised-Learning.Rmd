---
title: "Bayesian Supervised Learning in StochTree"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Supervised-Learning}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This interface is not necessarily designed for performance or simplicity
--- rather the intent is to provide a "prototype" interface to the C++
code that doesn't require modifying any C++.

To give one (simplified) example, rather than running
`sample_sigma2_one_iteration`, a researcher might prototype an
alternative global variance sampler in R and pass the updated global
variance parameter back to the forest sampler for another Gibbs
iteration.

To begin, load the stochtree package and set a seed for replicability.

```{r setup}
# Load library
library(stochtree)

# Set seed
random_seed = 1234
set.seed(random_seed)
```

Now, we simulate some straightforward data (in this case, a partitioned
linear model).

```{r data}
# Generate the data
n <- 500
p_X <- 10
p_W <- 1
X <- matrix(runif(n*p_X), ncol = p_X)
W <- matrix(runif(n*p_W), ncol = p_W)
f_XW <- (
    ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5*W[,1]) + 
    ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5*W[,1]) + 
    ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5*W[,1]) + 
    ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5*W[,1])
)
y <- f_XW + rnorm(n, 0, 1)

# Standardize outcome
y_bar <- mean(y)
y_std <- sd(y)
resid <- (y-y_bar)/y_std
```

Set some parameters that inform the forest and variance parameter
samplers

```{r hyperparameters}
alpha <- 0.9
beta <- 1.25
min_samples_leaf <- 1
num_trees <- 100
cutpoint_grid_size = 100
global_variance_init = 1.
tau_init = 0.5
leaf_prior_scale = matrix(c(tau_init), ncol = 1)
nu <- 4
lambda <- 0.5
a_leaf <- 2.
b_leaf <- 0.5
leaf_regression <- T
feature_types <- as.integer(rep(0, p_X)) # 0 = numeric
var_weights <- rep(1/p_X, p_X)
# var_1_prob <- 0.9
# var_weights <- c(var_1_prob, rep((1-var_1_prob)/(p_X - 1), p_X - 1))
```

Initialize R-level access to the C++ classes needed to sample our model

```{r external_pointers}
# Data
if (leaf_regression) {
    data_ptr <- create_forest_dataset(X, W)
    outcome_model_type <- 1
} else {
    data_ptr <- create_forest_dataset(X)
    outcome_model_type <- 0
}
outcome_ptr <- create_column_vector(resid)

# Random number generator (std::mt19937)
rng_ptr <- random_number_generator(random_seed)

# Sampling data structures
tree_prior_ptr <- tree_prior(alpha, beta, min_samples_leaf)
tracker_ptr <- forest_tracker(data_ptr, feature_types, num_trees, n)

# Container of forest samples
if (leaf_regression) {
    forest_samples_ptr <- forest_container(num_trees, 1, F)
} else {
    forest_samples_ptr <- forest_container(num_trees, 1, T)
}
```

Prepare to run the sampler

```{r sampling_container}
num_warmstart <- 10
num_mcmc <- 100
num_samples <- num_warmstart + num_mcmc
global_var_samples <- c(global_variance_init, rep(0, num_samples))
leaf_scale_samples <- c(tau_init, rep(0, num_samples))
```

Run the grow-from-root sampler to "warm-start" BART

```{r gfr}
for (i in 1:num_warmstart) {
    sample_model_one_iteration(data_ptr, outcome_ptr, forest_samples_ptr, tracker_ptr, tree_prior_ptr,
                               rng_ptr, feature_types, outcome_model_type, leaf_prior_scale, var_weights,
                               global_var_samples[i], cutpoint_grid_size, gfr = T)
    global_var_samples[i+1] <- sample_sigma2_one_iteration(outcome_ptr, rng_ptr, nu, lambda)
    leaf_scale_samples[i+1] <- sample_tau_one_iteration(forest_samples_ptr, rng_ptr, a_leaf, b_leaf, i-1)
    leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]
}
```

Pick up from the last GFR forest (and associated global variance / leaf
scale parameters) with an MCMC sampler

```{r mcmc}
for (i in (num_warmstart+1):num_samples) {
    sample_model_one_iteration(data_ptr, outcome_ptr, forest_samples_ptr, tracker_ptr, tree_prior_ptr, 
                               rng_ptr, feature_types, outcome_model_type, leaf_prior_scale, var_weights, 
                               global_var_samples[i], cutpoint_grid_size, gfr = F)
    global_var_samples[i+1] <- sample_sigma2_one_iteration(outcome_ptr, rng_ptr, nu, lambda)
    leaf_scale_samples[i+1] <- sample_tau_one_iteration(forest_samples_ptr, rng_ptr, a_leaf, b_leaf, i-1)
    leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]
}
```

Predict and rescale samples

```{r predict}
# Forest predictions
preds <- predict_forest(forest_samples_ptr, data_ptr)*y_std + y_bar

# Global error variance
sigma_samples <- sqrt(global_var_samples)*y_std
```

Inspect the XBART results

```{r xbart_plot}
plot(leaf_scale_samples[1:num_warmstart], ylab="tau")
plot(sigma_samples[1:num_warmstart], ylab="sigma^2")
plot(rowMeans(preds[,1:num_warmstart]), y, pch=16, cex=0.75, xlab = "pred", ylab = "actual"); abline(0,1,col="red",lty=2,lwd=2.5)
```

Inspect the warm start BART results

```{r warm_start_plot}
plot(leaf_scale_samples[(num_warmstart+1):num_samples], ylab="tau")
plot(sigma_samples[(num_warmstart+1):num_samples], ylab="sigma^2")
plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16, cex=0.75, xlab = "pred", ylab = "actual"); abline(0,1,col="red",lty=2,lwd=2.5)
```
