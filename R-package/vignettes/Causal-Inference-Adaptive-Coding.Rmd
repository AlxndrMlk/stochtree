---
title: "Customized Causal Machine Learning in StochTree"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Supervised-Learning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This interface is not necessarily designed for performance or simplicity
--- rather the intent is to provide a "prototype" interface to the C++
code that doesn't require modifying any C++.

To give one (simplified) example, rather than running 
`sample_sigma2_one_iteration`, a researcher might prototype an alternative 
global variance sampler in R and pass the updated global variance 
parameter back to the forest sampler for another Gibbs iteration.

To begin, load the stochtree package and set a seed for replicability.

```{r setup}
# Load library
library(stochtree)

# Set seed
random_seed = 1234
set.seed(random_seed)
```

Now, we simulate some straightforward data (in this case, a partitioned 
linear model).

```{r data}
# Generate the data
n <- 1000
p_X <- 10
p_W <- 1
X <- matrix(runif(n*p_X), ncol = p_X)
pi_X <- X[,1]*0.5 + 0.25
Z <- as.numeric(matrix(rbinom(n, 1, pi_X), ncol = 1))
mu_X <- (X[,1]*0.5 + 0.25)*5
tau_X <- X[,2]*3 - X[,3]*1
f_XX <- mu_X + tau_X*Z
y <- f_XX + rnorm(n, 0, 1)

# Standardize outcome
y_bar <- mean(y)
y_std <- sd(y)
resid <- (y-y_bar)/y_std
```

Set some parameters that inform the forest and variance parameter samplers

```{r hyperparameters}
# Mu forest
alpha_mu <- 0.9
beta_mu <- 1.25
min_samples_leaf_mu <- 1
num_trees_mu <- 200
cutpoint_grid_size_mu = 100
tau_init_mu = 0.5
leaf_prior_scale_mu = matrix(c(tau_init_mu), ncol = 1)
a_leaf_mu <- 2.
b_leaf_mu <- 0.5
leaf_regression_mu <- F

# Tau forest
alpha_tau <- 0.75
beta_tau <- 2.0
min_samples_leaf_tau <- 1
num_trees_tau <- 50
cutpoint_grid_size_tau = 100
tau_init_tau = 0.5
leaf_prior_scale_tau = matrix(c(tau_init_tau), ncol = 1)
a_leaf_tau <- 2.
b_leaf_tau <- 0.5
leaf_regression_tau <- T

# Common parameters
nu <- 4
lambda <- 0.5
global_variance_init = 1.
```

Initialize R-level access to the C++ classes needed to sample our model

```{r external_pointers}
# Data
if (leaf_regression_mu) {
    data_ptr_mu <- create_forest_dataset(cbind(X,pi_X), Z)
    outcome_model_type_mu <- 1
} else {
    data_ptr_mu <- create_forest_dataset(cbind(X,pi_X))
    outcome_model_type_mu <- 0
}
if (leaf_regression_tau) {
    data_ptr_tau <- create_forest_dataset(X, Z)
    outcome_model_type_tau <- 1
} else {
    data_ptr_tau <- create_forest_dataset(X)
    outcome_model_type_tau <- 0
}
outcome_ptr <- create_column_vector(resid)
feature_types_mu <- as.integer(c(rep(0, p_X), 0)) # 0 = numeric
var_weights_mu <- rep(1/(p_X+1), p_X+1)
feature_types_tau <- as.integer(rep(0, p_X)) # 0 = numeric
var_weights_tau <- rep(1/p_X, p_X)

# Random number generator (std::mt19937)
rng_ptr <- random_number_generator(random_seed)

# Sampling data structures
tree_prior_ptr_mu <- tree_prior(alpha_mu, beta_mu, min_samples_leaf_mu)
tracker_ptr_mu <- forest_tracker(data_ptr_mu, feature_types_mu, num_trees_mu, n)
tree_prior_ptr_tau <- tree_prior(alpha_tau, beta_tau, min_samples_leaf_tau)
tracker_ptr_tau <- forest_tracker(data_ptr_tau, feature_types_tau, num_trees_tau, n)

# Container of forest samples
if (leaf_regression_mu) {
    forest_samples_ptr_mu <- forest_container(num_trees_mu, 1, F)
} else {
    forest_samples_ptr_mu <- forest_container(num_trees_mu, 1, T)
}
if (leaf_regression_tau) {
    forest_samples_ptr_tau <- forest_container(num_trees_tau, 1, F)
} else {
    forest_samples_ptr_tau <- forest_container(num_trees_tau, 1, T)
}
```

Prepare to run the sampler

```{r sampling_container}
num_warmstart <- 10
num_mcmc <- 100
num_samples <- num_warmstart + num_mcmc
global_var_samples <- c(global_variance_init, rep(0, num_samples))
leaf_scale_samples_mu <- c(tau_init_mu, rep(0, num_samples))
leaf_scale_samples_tau <- c(tau_init_tau, rep(0, num_samples))
```

Run the grow-from-root sampler to "warm-start" BART

```{r gfr}
for (i in 1:num_warmstart) {
    # Sample the prognostic forest
    sample_model_one_iteration(data_ptr_mu, outcome_ptr, forest_samples_ptr_mu, tracker_ptr_mu, tree_prior_ptr_mu,
                               rng_ptr, feature_types_mu, outcome_model_type_mu, leaf_prior_scale_mu, var_weights_mu,
                               global_var_samples[i], cutpoint_grid_size_mu, gfr = T)
    leaf_scale_samples_mu[i+1] <- sample_tau_one_iteration(forest_samples_ptr_mu, rng_ptr, a_leaf_mu, b_leaf_mu, i-1)
    leaf_prior_scale_mu[1,1] <- leaf_scale_samples_mu[i+1]
    
    # Sample the treatment forest
    sample_model_one_iteration(data_ptr_tau, outcome_ptr, forest_samples_ptr_tau, tracker_ptr_tau, tree_prior_ptr_tau,
                               rng_ptr, feature_types_tau, outcome_model_type_tau, leaf_prior_scale_tau, var_weights_tau,
                               global_var_samples[i], cutpoint_grid_size_tau, gfr = T)
    leaf_scale_samples_tau[i+1] <- sample_tau_one_iteration(forest_samples_ptr_tau, rng_ptr, a_leaf_tau, b_leaf_tau, i-1)
    leaf_prior_scale_tau[1,1] <- leaf_scale_samples_tau[i+1]
    
    # Sample global variance parameter
    global_var_samples[i+1] <- sample_sigma2_one_iteration(outcome_ptr, rng_ptr, nu, lambda)
}
```

Pick up from the last GFR forest (and associated global variance / leaf scale parameters) with an MCMC sampler

```{r mcmc}
for (i in (num_warmstart+1):num_samples) {
    # Sample the prognostic forest
    sample_model_one_iteration(data_ptr_mu, outcome_ptr, forest_samples_ptr_mu, tracker_ptr_mu, tree_prior_ptr_mu,
                               rng_ptr, feature_types_mu, outcome_model_type_mu, leaf_prior_scale_mu, var_weights_mu,
                               global_var_samples[i], cutpoint_grid_size_mu, gfr = F)
    leaf_scale_samples_mu[i+1] <- sample_tau_one_iteration(forest_samples_ptr_mu, rng_ptr, a_leaf_mu, b_leaf_mu, i-1)
    leaf_prior_scale_mu[1,1] <- leaf_scale_samples_mu[i+1]
    
    # Sample the treatment forest
    sample_model_one_iteration(data_ptr_tau, outcome_ptr, forest_samples_ptr_tau, tracker_ptr_tau, tree_prior_ptr_tau,
                               rng_ptr, feature_types_tau, outcome_model_type_tau, leaf_prior_scale_tau, var_weights_tau,
                               global_var_samples[i], cutpoint_grid_size_tau, gfr = F)
    leaf_scale_samples_tau[i+1] <- sample_tau_one_iteration(forest_samples_ptr_tau, rng_ptr, a_leaf_tau, b_leaf_tau, i-1)
    leaf_prior_scale_tau[1,1] <- leaf_scale_samples_tau[i+1]
    
    # Sample global variance parameter
    global_var_samples[i+1] <- sample_sigma2_one_iteration(outcome_ptr, rng_ptr, nu, lambda)
}
```

Predict and rescale samples

```{r prediction}
# Forest predictions
mu_preds <- predict_forest(forest_samples_ptr_mu, data_ptr_mu)*y_std + y_bar
tau_preds <- predict_forest_raw(forest_samples_ptr_tau, data_ptr_tau)*y_std

# Global error variance
sigma_samples <- sqrt(global_var_samples)*y_std
```

Inspect the XBART results

```{r xbcf_plot}
plot(sigma_samples[1:num_warmstart], ylab="sigma^2")
plot(rowMeans(mu_preds[,1:num_warmstart]), mu_X, pch=16, cex=0.75, xlab = "pred", ylab = "actual", main = "prognostic term");
abline(0,1,col="red",lty=2,lwd=2.5)
plot(rowMeans(tau_preds[,1:num_warmstart]), tau_X, pch=16, cex=0.75, xlab = "pred", ylab = "actual", main = "treatment effect term"); abline(0,1,col="red",lty=2,lwd=2.5)
```

Inspect the warm start BART results

```{r warm_start_plot}
plot(sigma_samples[(num_warmstart+1):num_samples], ylab="sigma^2")
plot(rowMeans(mu_preds[,(num_warmstart+1):num_samples]), mu_X, pch=16, cex=0.75, xlab = "pred", ylab = "actual", main = "prognostic term");
abline(0,1,col="red",lty=2,lwd=2.5)
plot(rowMeans(tau_preds[,(num_warmstart+1):num_samples]), tau_X, pch=16, cex=0.75, xlab = "pred", ylab = "actual", main = "treatment effect term"); abline(0,1,col="red",lty=2,lwd=2.5)
```
